# Fruit Ripeness & Disease Detection

A local Flask demo that detects fruits and classifies ripeness or disease using Ultralytics YOLO and image-classification models. The app supports live detection (browser webcam or server MJPEG stream) and image uploads for fruit-specific classifiers (banana, mango, pomegranate).

This repository is intended as a privacy-preserving demo: images stay on your machine and inference runs locally using PyTorch/Ultralytics models.

---

## Features

- Live detection from your webcam (two modes):
  - Browser Camera: uses client-side `getUserMedia` for camera access (recommended)
  - Server Stream: server-side MJPEG stream using OpenCV (useful if the server host has an attached camera)
- Object detection with YOLO for fruit localization
- Fruit-specific classification models for disease/ripeness (banana, mango, pomegranate)
- Upload pages with drag & drop and preview
- Lightweight and offline-capable if weights are present locally

---

## Repository structure (important files)

- `app_4.py` — main Flask application and detection endpoints
- `templates/` — Jinja2 HTML templates used by the app (home, live, upload/result pages)
- `static/` — CSS and JS assets used by the UI
- `weights/`, `train*/weights/` — places where model `.pt` files are expected (not checked in)
- `requirements.txt` — Python dependencies

---

## Prerequisites

- Python 3.8+ (3.10/3.11 recommended)
- A modern web browser (Chrome, Edge, Firefox). For browser camera: open `http://127.0.0.1:5000` on the same machine and allow camera access when prompted.
- (Optional) A CUDA-capable GPU if you want faster inference and installed CUDA-enabled `torch` build.

Note: Install the correct `torch` wheel for your platform/GPU to get best performance.

---

## Setup (Windows PowerShell)

1. Create a virtual environment and activate it (PowerShell):

```powershell
python -m venv .venv
. .venv\Scripts\Activate.ps1
```

2. Install dependencies:

```powershell
pip install -r requirements.txt
```

If you want GPU support, install the appropriate `torch` wheel before installing the rest of the requirements. Example (adjust CUDA version):

```powershell
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt
```

3. Place your model weight files (`.pt`) where the app expects them. Common locations used by the app:

- `weights/best.pt` or `weights_3/best.pt`
- `train/weights/best.pt`
- `train2/weights/best.pt`
- `train4/weights/best.pt`

The app will attempt to load models from a small set of common paths; if it can't find them it will continue to run but the UI will show "Missing" badges and detection will return no results.

---

## Run the app

Start the Flask app from the project root (inside the activated venv):

```powershell
python app_4.py
```

Open your browser at: http://127.0.0.1:5000

---

## UI Overview

- **Home**: project description, quick-start steps, and model status badges showing whether each model was loaded.
- **Live Detection**: two modes selectable at the top:
  - *Browser Camera* — recommended: the page asks for permission to use your webcam. Captures frames client-side and sends them to the server for inference.
  - *Server Stream* — uses a server-side MJPEG stream generated by OpenCV. The app attempts to open camera indices `0..3` automatically; if none are available it shows a placeholder image.
- **Disease Detection**: page linking to individual upload pages for Banana, Mango, and Pomegranate. Upload pages support drag-and-drop and show results in the right panel.
- **Uploaded Image**: shows the input image and a list of predicted classes and confidences.

---

## API endpoints (demo)

- `GET /` — homepage
- `GET /fruit_detection` — live detection page
- `GET /video_feed` — MJPEG server stream (multipart/x-mixed-replace)
- `POST /detect_objects` — accepts JSON `{ image_data: '<dataURL>' }` and returns detected objects as JSON. Response format: an array of objects with keys `class`, `bbox`, `confidence`.
- `POST /banana_detection`, `/mango_detection`, `/pomogranate_detection` — upload routes that return the rendered result page

These endpoints are for the demo UI. For programmatic clients, call `/detect_objects` with a base64 data URL in the `image_data` field.

---

## Troubleshooting

- **Camera not working in browser**:
  - Ensure the site is loaded on `localhost` or `127.0.0.1` (browsers allow getUserMedia on `localhost` without HTTPS).
  - Allow camera access when prompted. Check the browser site permissions (lock icon near the URL).
  - If multiple apps use the camera, close other apps (Camera app, Zoom, etc.).

- **Server-side camera streaming shows placeholder "Camera unavailable"**:
  - Ensure the Python process has permission to access the camera.
  - Check OS privacy settings (Windows: Settings → Privacy → Camera).
  - Test OpenCV camera access in the venv:

```python
import cv2
for i in range(4):
    cap = cv2.VideoCapture(i)
    print(i, 'opened=', cap.isOpened())
    if cap.isOpened():
        ret, frame = cap.read()
        print('  read', ret, 'shape', None if frame is None else frame.shape)
        cap.release()
        break
    try:
        cap.release()
    except:
        pass
```

- **Model not loaded / FileNotFoundError when starting app**:
  - Place your `.pt` weight files in the `weights/` or `train*/weights/` folders. The app logs which paths it tried. Example expected paths: `train2/weights/best.pt`, `weights_3/best.pt`.

- **Detection returns empty list**:
  - Check the Flask console for inference errors. If the model loaded but inference failed, inspect the stack trace to determine cause (version mismatch with ultralytics or torch, etc.).

---

## Performance tips

- Use a GPU-enabled PyTorch build for much faster inference.
- Lower the MJPEG quality or resolution to reduce CPU usage.
- For higher FPS, prefer client-side capture (browser) + server inference API, and draw boxes client-side with returned JSON.

---

## Development notes

- UI templates are in `templates/`. Static assets are in `static/`.
- `generate_frames()` in `app_4.py` tries camera indices `0..3` — change this range if your camera is at a different index.
- Detection code is written to tolerate different Ultralytics result shapes (classification-style or detection-style), but you may need to adjust parsing if your models return custom outputs.

---

## Extension ideas (I can implement)

- Add a `/capture` endpoint for single-frame server captures (cleaner server capture from attached camera).
- Add client-side bounding-box drawing using detection JSON to overlay boxes on the live video element.
- Add a camera-index selector UI for the server stream to pick the correct camera if auto-detection doesn't find it.

---

## License & Notes

This repository does not include third-party model weights. Use and distribute your own weights following their licenses. The project code is provided as-is. Add a `LICENSE` file if you want to release under a specific open-source license.

---

If you want, tell me which feature you want next and I will implement it (client overlays, `/capture` endpoint, camera index selector, or improved diagnostics panel).
# Fruit-Disease-Detection-